[
    {
        "name": "High LLM Latency",
        "type": "metric alert",
        "query": "avg(last_5m):avg:voicedoc.request.latency_ms{service:voicedoc-agent,traffic_type:user} > 10000",
        "message": "## LLM Latency Spike Detected\n\n**Signal**: Gemini model response time exceeded 10s.\n\n### ðŸ”§ Runbook for AI Engineer:\n1. Check Google Cloud Console for Vertex AI quotas or regional outages.\n2. Review recently uploaded documents for excessive length or complexity (check `file.size` tag in traces).\n3. Investigate if 'Expressive Mode' (two-call process) is introducing overhead.\n\n@cases-voicedoc-incidents",
        "tags": [
            "service:voicedoc-agent",
            "team:ai"
        ],
        "options": {
            "thresholds": {
                "critical": 10000,
                "warning": 7000
            }
        }
    },
    {
        "name": "Gemini API Error Spike",
        "type": "metric alert",
        "query": "sum(last_5m):sum:voicedoc.request.errors{service:voicedoc-agent,traffic_type:user}.as_count() > 5",
        "message": "## Gemini API Error Surge\n\n**Signal**: Multiple failed attempts to generate responses.\n\n### ðŸ”§ Runbook for AI Engineer:\n1. Examine backend logs for `[Route] âŒ Fatal error` or `[Stream] âŒ Generator error`.\n2. Verify `VERTEX_PROJECT_ID` and credentials in Cloud Run environment.\n3. Check if Firestore RUM retrieval is failing (check `rag.error` tag).\n\n@cases-voicedoc-incidents",
        "tags": [
            "service:voicedoc-agent",
            "priority:high"
        ],
        "options": {
            "thresholds": {
                "critical": 5
            }
        }
    },
    {
        "name": "LLM Token Burn Rate",
        "type": "metric alert",
        "query": "sum(last_1h):sum:voicedoc.llm.total_tokens{service:voicedoc-agent,traffic_type:user}.as_count() > 500000",
        "message": "## High Token Consumption Warning\n\n**Signal**: Token usage > 500k in 1 hour.\n\n### ðŸ”§ Runbook for AI Engineer:\n1. Monitor `llm.total_tokens` by user session in Datadog RUM.\n2. Verify if a specific document is causing infinite loops or massive RAG retrieval results.\n3. Check for potential prompt injection attempts in `llm.query` tags.\n\n@cases-voicedoc-incidents",
        "tags": [
            "service:voicedoc-agent",
            "type:cost"
        ],
        "options": {
            "thresholds": {
                "critical": 500000
            }
        }
    }
]